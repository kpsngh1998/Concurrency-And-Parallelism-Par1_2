Course Title: Parallel and Concurrent Programming with Java 1

Description: Parallel programming unlocks a programâ€™s ability to execute multiple instructions simultaneously. It increases the overall processing throughput and is key to writing faster and more efficient applications. This training course introduces the basics of parallel programming in Java, providing the foundational knowledge you need to write more efficient, performant code. Instructors Barron and Olivia Stone explain concepts like threading and mutual exclusion in a fun and informative way, relating them to everyday activities you perform in the kitchen. To cement the ideas, they demo them in action using Java. Each lesson is short and practical, driving home the theory with hands-on techniques.


***********************************************
Chapter: 2. Threads and Processes
***********************************************


-----------------------------------------------
Video: Execution scheduling: Java
-----------------------------------------------
Note Time:         Note Text:                     

0:00:07            - Just because a program is structured to have multiple threads or processes does not mean they'll necessarily execute in parallel. A concept that's closely related to parallel execution but often gets confused with it is concurrency. Concurrency refers to the ability of an algorithm or program to be broken into different parts that can be executed out of order or partially out of order without affecting the end result. Concurrency is about how a program is structured and the composition of independently executing processes. Consider this recipe to make a salad, which includes several steps that involve slicing and chopping vegetables. We can decompose those steps into a collection of concurrent tasks because the relative order in which we do them doesn't matter. They're order independent. To keep things simple, let's just focus on two of those tasks for now. I'll chop onions. - And I'll slice cucumbers. This knife represents our computer's processor. We only have one knife, so this is a single core processor. And only one of us will be able to execute our vegetable chopping routine at any given time. - We'll have to take turns. You go first. - Thanks. My thread will use a processor to execute and slice some cucumbers. Then, after a bit, we'll swap places. - Now my thread gets some time to execute and slice onions. - I want to slice now. - So we'll swap places again, and we'll keep on doing this until we're both done. In this scenario, we're running concurrently because our two independent processes overlap in time. However, since we only have a single processor, only one of us will actually be executing at any instant in time. If we swap places and take turns more frequently, it might create the illusion that we're executing simultaneously on our single processor, but this is not true parallel execution. - To actually execute in parallel, we need parallel hardware. In our kitchen, that means another knife and cutting board, a second processor. But in regards to computers, parallel hardware can come in a variety of forms. Most modern processors used in things like desktop computers and cellphones have multiple processing cores. Graphics processing units, or GPUs, contain hundreds, or even thousands, of specialized cores working in parallel to make amazing graphics that you see on the screen. And computer clusters distribute their processing across multiple systems. Since we've structured ourselves as concurrent operations, I can begin slicing cucumbers with this processor. - While I cut onions with the other one. Now we're actually executing in parallel because we're both executing at the same time. And, as a result, we're able to finish the job faster. Concurrency is about the structure of a program, being able to deal with multiple things at once, whereas parallelism is about simultaneous execution, actually doing multiple things at once. Those things could be related, like chopping vegetables, but they don't have to be. Concurrency enables a program to execute in parallel, given the necessary hardware. But a concurrent program is not inherently parallel. - And programs may not always benefit from parallel execution. For example, the software drivers that handle I/O devices, like a mouse, keyboard, and hard drive, need to execute concurrently. They're managed by the operating system as independent things that get executed, as needed. In a multi-core system, the execution of those drivers might get split amongst the available processors. However, since I/O operations occur rather infrequently, relative to the speed at which computer operates, we don't really gain anything from parallel execution. Those sparse independent tasks can run just fine on a single processor, and we wouldn't feel a difference. Concurrent programming is useful for I/O-dependent tasks like graphical user interfaces. When the user clicks a button to execute an operation, that might take a while. To avoid locking up the user interface until it's completed, we can run the operation in a separate concurrent thread. This leaves the thread that's running the UI free to accept new inputs. - That sort of I/O-dependent task is a good use case for concurrency. Parallel processing really becomes useful for computationally intensive tasks, such as calculating the result of multiplying two matrices together. When large math operations can be divided into independent subparts, executing those parts in parallel on separate processors can really speed things up. 

0:00:08            - Just because a program is structured to have multiple threads or processes does not mean they'll necessarily execute in parallel. A concept that's closely related to parallel execution but often gets confused with it is concurrency. Concurrency refers to the ability of an algorithm or program to be broken into different parts that can be executed out of order or partially out of order without affecting the end result. Concurrency is about how a program is structured and the composition of independently executing processes. Consider this recipe to make a salad, which includes several steps that involve slicing and chopping vegetables. We can decompose those steps into a collection of concurrent tasks because the relative order in which we do them doesn't matter. They're order independent. To keep things simple, let's just focus on two of those tasks for now. I'll chop onions. - And I'll slice cucumbers. This knife represents our computer's processor. We only have one knife, so this is a single core processor. And only one of us will be able to execute our vegetable chopping routine at any given time. - We'll have to take turns. You go first. - Thanks. My thread will use a processor to execute and slice some cucumbers. Then, after a bit, we'll swap places. - Now my thread gets some time to execute and slice onions. - I want to slice now. - So we'll swap places again, and we'll keep on doing this until we're both done. In this scenario, we're running concurrently because our two independent processes overlap in time. However, since we only have a single processor, only one of us will actually be executing at any instant in time. If we swap places and take turns more frequently, it might create the illusion that we're executing simultaneously on our single processor, but this is not true parallel execution. - To actually execute in parallel, we need parallel hardware. In our kitchen, that means another knife and cutting board, a second processor. But in regards to computers, parallel hardware can come in a variety of forms. Most modern processors used in things like desktop computers and cellphones have multiple processing cores. Graphics processing units, or GPUs, contain hundreds, or even thousands, of specialized cores working in parallel to make amazing graphics that you see on the screen. And computer clusters distribute their processing across multiple systems. Since we've structured ourselves as concurrent operations, I can begin slicing cucumbers with this processor. - While I cut onions with the other one. Now we're actually executing in parallel because we're both executing at the same time. And, as a result, we're able to finish the job faster. Concurrency is about the structure of a program, being able to deal with multiple things at once, whereas parallelism is about simultaneous execution, actually doing multiple things at once. Those things could be related, like chopping vegetables, but they don't have to be. Concurrency enables a program to execute in parallel, given the necessary hardware. But a concurrent program is not inherently parallel. - And programs may not always benefit from parallel execution. For example, the software drivers that handle I/O devices, like a mouse, keyboard, and hard drive, need to execute concurrently. They're managed by the operating system as independent things that get executed, as needed. In a multi-core system, the execution of those drivers might get split amongst the available processors. However, since I/O operations occur rather infrequently, relative to the speed at which computer operates, we don't really gain anything from parallel execution. Those sparse independent tasks can run just fine on a single processor, and we wouldn't feel a difference. Concurrent programming is useful for I/O-dependent tasks like graphical user interfaces. When the user clicks a button to execute an operation, that might take a while. To avoid locking up the user interface until it's completed, we can run the operation in a separate concurrent thread. This leaves the thread that's running the UI free to accept new inputs. - That sort of I/O-dependent task is a good use case for concurrency. Parallel processing really becomes useful for computationally intensive tasks, such as calculating the result of multiplying two matrices together. When large math operations can be divided into independent subparts, executing those parts in parallel on separate processors can really speed things up. 


-----------------------------------------------
Video: Thread attributes: Java demo
-----------------------------------------------
Note Time:         Note Text:                     

0:00:51            Execution scheduling: Java
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] To demonstrate how scheduling can impact execution, I've written this Java program that creates two threads named Baron and Olivia that continuously chop vegetables for about one second. Each instance of the Vegetable Chopper class defined on line five will run as a separate thread. Inside of a classes run method, which is on line 14, it uses a y loop that will continuously run as long as the class variable name Chopping is true. And for each loop iteration, it will print a message and then increment the value of an instance variable named Vegetable Count. Down in the program's main method on line 23, I instantiate two vegetable chopper threads named Baron and Olivia and then I start them both chopping vegetables at roughly the same time. I let them both run for about one second before setting the vegetable chopper classes variable named Chopping to false on line 30 which stops them both. Then finally at the end of the program, I print a message that indicates how many vegetables Baron and Olivia each chopped, which will indicate how much each of their two threads were scheduled to execute. If I run this example, I see in the output messages saying that Baron and Olivia chopped vegetables. And then at the end, I can see Baron chopped about 61,000 vegetables, and Olivia only chopped about 58,000 vegetables. So even though they were both started and stopped at roughly the same time, these two threads got different amounts of time to execute. Now, let me run that one more time. They chop again for about one second, and this time there had only chopped 59,000 vegetables, whereas Olivia chopped more with 66,000. You can see that each time we run this, we're getting a different result. So scheduling is not always consistent, so you shouldn't rely on it for threads to be executed in equal amount of time or in a particular order. 

0:00:52            Execution scheduling: Java
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] To demonstrate how scheduling can impact execution, I've written this Java program that creates two threads named Baron and Olivia that continuously chop vegetables for about one second. Each instance of the Vegetable Chopper class defined on line five will run as a separate thread. Inside of a classes run method, which is on line 14, it uses a y loop that will continuously run as long as the class variable name Chopping is true. And for each loop iteration, it will print a message and then increment the value of an instance variable named Vegetable Count. Down in the program's main method on line 23, I instantiate two vegetable chopper threads named Baron and Olivia and then I start them both chopping vegetables at roughly the same time. I let them both run for about one second before setting the vegetable chopper classes variable named Chopping to false on line 30 which stops them both. Then finally at the end of the program, I print a message that indicates how many vegetables Baron and Olivia each chopped, which will indicate how much each of their two threads were scheduled to execute. If I run this example, I see in the output messages saying that Baron and Olivia chopped vegetables. And then at the end, I can see Baron chopped about 61,000 vegetables, and Olivia only chopped about 58,000 vegetables. So even though they were both started and stopped at roughly the same time, these two threads got different amounts of time to execute. Now, let me run that one more time. They chop again for about one second, and this time there had only chopped 59,000 vegetables, whereas Olivia chopped more with 66,000. You can see that each time we run this, we're getting a different result. So scheduling is not always consistent, so you shouldn't rely on it for threads to be executed in equal amount of time or in a particular order. 

0:00:52            Execution scheduling: Java
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] To demonstrate how scheduling can impact execution, I've written this Java program that creates two threads named Baron and Olivia that continuously chop vegetables for about one second. Each instance of the Vegetable Chopper class defined on line five will run as a separate thread. Inside of a classes run method, which is on line 14, it uses a y loop that will continuously run as long as the class variable name Chopping is true. And for each loop iteration, it will print a message and then increment the value of an instance variable named Vegetable Count. Down in the program's main method on line 23, I instantiate two vegetable chopper threads named Baron and Olivia and then I start them both chopping vegetables at roughly the same time. I let them both run for about one second before setting the vegetable chopper classes variable named Chopping to false on line 30 which stops them both. Then finally at the end of the program, I print a message that indicates how many vegetables Baron and Olivia each chopped, which will indicate how much each of their two threads were scheduled to execute. If I run this example, I see in the output messages saying that Baron and Olivia chopped vegetables. And then at the end, I can see Baron chopped about 61,000 vegetables, and Olivia only chopped about 58,000 vegetables. So even though they were both started and stopped at roughly the same time, these two threads got different amounts of time to execute. Now, let me run that one more time. They chop again for about one second, and this time there had only chopped 59,000 vegetables, whereas Olivia chopped more with 66,000. You can see that each time we run this, we're getting a different result. So scheduling is not always consistent, so you shouldn't rely on it for threads to be executed in equal amount of time or in a particular order. 

0:00:59            Now, one thing that java threads do not have is a way to directly identify their parent thread. When a new thread is created, it's parent is used to set properties like the threads priority and it's demon status, which we'll cover later. But the child does not maintain a reference to it's parent thread. One reason for that is that it enables the parent thread to be garbage collected by the jvm to reclaim memory, which would not be possible if the child thread was holding a reference to it. 


-----------------------------------------------
Video: Daemon thread
-----------------------------------------------
Note Time:         Note Text:                     

0:00:22            Daemon thread
Selecting transcript lines in this section will navigate to timestamp in the video
- We often create threads to provide some sort of service, or perform a periodic task in support of the main program. A common example of that is garbage collection. A garbage collector is a form of automatic memory management that runs in the background and attempts to reclaim garbage, or memory that's no longer being used by the program. Many languages include garbage collection as a standard part of their run time environment, but for this demonstration, I'll spawn my own new thread to handle garbage collection. - Man, what a mess. - Olivia is a separate child thread that will execute independently of my main thread. So, I can continue doing what I'm doing here, getting my soup ingredients ready. - While I try to reclaim some memory, or counter space, by clearing out Baron's garbage. - This set-up, with Olivia running as a separate thread to provide that garbage collection service, will work fine until I'm ready to finish executing. Bam, now my soup's spiced and ready, my main thread is done executing, and I'm ready to exit the program. But I can't. - Because I'm still running. Since Baron spawned me as a normal child thread, he won't be able to exit until I've terminated. - And since Olivia's thread is designed to collect garbage in a continuous loop, she'll never exit. I'll be stuck here waiting forever, and this process will never terminate. Threads that are performing background tasks, like garbage collection, can be detached from the main program by making them what's called a daemon thread. A daemon thread, which you may also hear pronounced as Damon, is a thread that will not prevent the program from exiting if it's still running. By default, new threads are usually spawned as non-daemon or normal threads, and you have to explicitly turn a thread into a daemon or background thread. Olivia, I forgot to tell you this earlier, but you're a daemon thread. - Oh man, you detached me. - When my main thread is finished executing and there aren't any non-daemon threads left running, this process can terminate. And Olivia's daemon thread will terminate with it. - Since I was terminated abruptly with the process, I didn't have a chance to gracefully shut down and stop what I was doing. That's fine in the case of a garbage collection routine, because all of the memory this process was using will get cleared as part of terminating it. But if I was doing some sort of IO operation, like writing to a file, then terminating in the middle of that operation could end up corrupting data. If you detach a thread to make it a background task, make sure it won't have any negative side-effects if it prematurely exits. 


-----------------------------------------------
Video: Daemon thread: Java demo
-----------------------------------------------
Note Time:         Note Text:                     

0:01:45            Daemon thread: Java demo
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] In this example program called daemon thread demo, I define a class called KitchenCleaner on line five which represents a periodic background task like garbage collection. The KitchenCleaner uses an infinite while loop on line seven to print a message that Olivia cleaned the kitchen, and will continuously repeat that message once a second. Down in the program's main method, I create and start a new KitchenCleaner thread on lines 19 and 20, then the main thread prints a series of messages that Barren is cooking, which are split up by some sleep statements and then finally a message that Barren is done on line 28. If I run this program, I see those messages displayed but after the main thread reaches the end and prints its final Barren is done message, the program doesn't exit because the KitchenCleaner thread is still going strong and will continue to run forever. So, I'll need to manually stop execution by clicking the little red square. To prevent the program from continuing to run like this, I'll set Olivia to be a daemon thread by using the setDaemon method and I'll pass it an input of true. Now when I run the program again, when the main thread is done executing, the KitchenCleaner is also terminated, so the process can exit. A few things to note about daemon threads in Java. When a new thread is created, it will inherit the daemon status from its parent. The main thread is a normal non-daemon thread. So by default, all threads that it creates will be non-daemon threads. You must use the setDaemon method to change a thread's daemon status before starting it. And finally, daemon threads do not gracefully exit like normal threads. When the JVM halts, any remaining daemon threads will be abandoned and that happens abruptly. It doesn't execute any finally blocks to close things down, it doesn't unwind the stack, the JVM just exits. And for that reason, daemon threads should be used sparingly. It's dangerous to use them for tasks that might perform any sort of IO operation because if the JVM exists while the daemon is in the middle of writing something, then you can end up with corrupted data. 


***********************************************
Chapter: 3. Mutual Exclusion
***********************************************


-----------------------------------------------
Video: Synchronized statement: Java demo
-----------------------------------------------
Note Time:         Note Text:                     

0:03:33            VIMP VIMP NOTE:                

0:03:46            VIMP VIMP : Synchronized statement: Java demo
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Rather than marking an entire method as synchronized, Java also makes it possible to synchronize sections of code with a synchronized statement. When creating an synchronized statement, you specify the object that will provide the intrinsic lock. Before a thread can execute the code contained within the synchronized statement, it must first acquire the intrinsic lock associated with the specified object and then when the thread is done, it will release its hold on that lock. To demonstrate creating a synchronized statement, I'll modify the data race example program that I have been using for the past few videos which has two concurrent threads incrementing a shared counter variable 10 million times each. I want to synchronize access to line 11 which increments the shared garlicCount variable. So I'll wrap that line in a synchronized statement and I'll give it the shopper.class object to use for the intrinsic lock. When I run this program both threads will be acquiring and releasing the same intrinsic locks associated with the shopper class before and after they increment the garlicCount. So I get the expected output value of 20 million. Now let's see what happens if instead of using the shopper class object, I use each instance of the shopper as the object for the synchronized statement. To do that, we'll replace shopper.class with the this keyword. Now when I run the program, I get an incorrect result because each of the two shopper threads is acquiring and releasing the intrinsic lock associated with their own instances. They're not synchronized to the same object now so the data race occurs. Something else that might seem like a good idea to do here would be to synchronize this statement on the garlicCount variable itself. But if I try to do that I get an error because I need to use an object for synchronization and garlicCount is currently just a primitive int variable defined on line seven, but that's an easy change to make. I'll just change int into an integer object. Now when I run this program the output is less than 20 million. The data race still exists. The problem now is that Java's integer object is immutable. Once you create a integer instance, you cannot change its value, but I'm doing just that on line 12 when I increment the garlicCount so what's really happening here is that every time a thread executes the garlicCount plus plus operation, Java instantiates a new integer object which will have a different object ID. So each time the thread loops back around and executes that synchronized statement, they'll actually be using a different object for the intrinsic lock. So they're not really synchronized at all. This is a sneaky little trap you can fall into so be careful when selecting the object to use for synchronized statements. Over the past few videos, we've looked at several different mechanisms to implement mutual exclusion in Java: locks, atomic variables, and synchronized methods or statements. So, which is best? In general, if you're using Java, synchronized statements and methods are easier to program with than locks. And they can prevent many common pit falls that can occur when using locks. So, if synchronized methods or statements will work for your needs, they're a good default option. That said, there may be times when you need to work with locks in more complex ways, perhaps acquiring and releasing a series of locks in a nested or hand over hand manner, and that's not possible with a synchronized statement, but locks do allow more flexibility to be acquired and released in different scopes and to be acquired and released in any order. But with that increased flexibility, comes additional responsibility and we'll look at why later in this course. 


***********************************************
Chapter: 4. Locks
***********************************************


-----------------------------------------------
Video: Reentrant lock
-----------------------------------------------
Note Time:         Note Text:                     

0:02:11            - Olivia and I have been using this pencil as a mutex. Only one person at a time can own or have a lock on it, and only that person can access our shared resource, this notepad. - If I attempt to lock the mutex while another thread has it, my thread will be blocked, and I need to wait until he unlocks it so it becomes available. - And if I attempt to lock the mutex, it doesn't appear to be available, so my thread will just have to wait too. - It's behind your ear. You already locked it. - Oh. Well my thread can't unlock the mutex while I'm blocked waiting on it, and I'll be waiting on the mutex forever because I'll never be able to unlock it. I'm stuck, and so are you. If a thread tries to lock a mutex that it's already locked, it'll enter into a waiting list for that mutex, which results in something called a deadlock, because no other thread can unlock that mutex. - There may be times when a program needs to lock a mutex multiple times before unlocking it. In that case, you should use a reentrant mutex to prevent this type of problem. A reentrant mutex is a particular type of mutex that can be locked multiple times by the same process or thread. Internally, the reentrant mutex keeps track of how many times it's been locked by the owning thread, and it has to be unlocked an equal number of times before another thread can lock it. If this pencil is a reentrant mutex, when I pick it up, I lock it. - Now Olivia's thread has a hold on the mutex, so she's the only one that can lock or unlock it. - Since the pencil is reentrant I can lock it again. Now the pencil has been locked twice by me, which means I'll have to unlock it twice to fully release my hold on it. If your program needs to lock a mutex multiple times, using a reentrant mutex may seem like an easy way to avoid a deadlock. But if you don't unlock the reentrant mutex the same number of times, you can still end up stuck. - I'm waiting. Thanks. Many programmers like using reentrant locks because it can make things easier. You don't need to worry as much about what's already been locked, and they make it easier to retrofit locks into existing code. As an example, say I have a function to increment a shared counter, and it uses a mutex to protect that operation. If later I create another function that uses the same mutex to protect some other section of code, and that section of code uses the increment counter function, since those functions are nested, when I execute my function it will end up locking the mutex twice before unlocking it. If I was using a regular non-reentrant lock, that would produce a deadlock, but with a reentrant mutex this works just fine. Now like many things in the world of programmers, there are some very strong opinions about whether reentrant locks are good or evil. Some opponents of using reentrant locks will argue that the example I just showed you should be refactored to avoid having nested locks by using a third function that increments the counter and only gets called from within a protected section. I'm not going to advocate either way on this debate. There are pros and cons to both sides. - One use case where reentrant locks are really needed is when writing a recursive function, that is, a function that calls itself. If the function makes a recursive call to itself from within a locked section, it will lock the mutex multiple times as it repeats itself, and then unlock the mutex an equal number of times as it returns and unwinds. Since a reentrant mutex can be used recursively like this, you'll often hear it referred to as a recursive mutex or a recursive lock. Different languages use different terms, but these all basically mean the same thing. 


-----------------------------------------------
Video: Try lock
-----------------------------------------------
Note Time:         Note Text:                     

0:01:49            Try lock
Selecting transcript lines in this section will navigate to timestamp in the video
- When multiple threads each have multiple tasks to perform, making those threads block and wait every time they attempt to acquire a lock that's already taken may not be necessary or efficient. Olivia and I are two threads doing several different tasks. My thread will be taking an inventory of the fridge to see what things we're running low on and then add those to the shopping list on our shared notepad. I'll go back and forth between those two tasks. - [Olivia] And my thread is searching through the newspaper for grocery coupons and then adding those items to the shared shopping list. Ooh, there are some good deals this week! Now that I've found some items that I want, I'll take the pencil, which is our mutex, to lock access to the shared notepad so I can add them. - I saw we're low on milk. So now I'll go to acquire the pencil. And I see Olivia has it. If I attempt to lock a mutex in a regular blocking fashion, my thread would enter a waiting state at this point, doing nothing until Olivia unlocks it. If I don't have anything else to do, so I can't continue with other things until after I've accessed the shared notepad, that's okay. It's what has to happen, but in this scenario, I do have other useful things to do that don't require the notepad. I can keep searching the fridge for other things we need. So, rather than using the standard locking method to acquire the mutex, I'll use what's called try lock, or try enter, which is a non-blocking version of the lock or acquire method. It returns immediately and one of two things will happen. If the mutex you're trying to lock is available, it will get locked, and the method with return TRUE. Otherwise, if the mutex is already possessed by another thread, the try lock method will immediately return FALSE. That return value of true or false lets the thread know whether or not it was successful in acquiring the lock. So, if I try to lock the pencil that Olivia currently has, I know immediately that my attempt has failed. So I can go back to searching the fridge. - There, I'm done writing for now. So I'll unlock the pencil and go back to searching for coupons. Since Barron wasn't blocked waiting for this mutex, it's just sitting unlocked available for anyone to take. Now, Barron likes to explain try lock with pencils and notepads. I think of it more like being at a house party with a bunch of your friends, your fellow threads. There's one restroom at the house that everyone at the party will need to use at some point. But only person can use it at a time. When you try to use it, and try opening the door, you realize it's locked because someone's already inside. You could stand there and wait until they come out or you could go back to the party, keep having fun, and try again later. 


***********************************************
Chapter: 5. Liveness
***********************************************


-----------------------------------------------
Video: Abandoned lock: Java demo
-----------------------------------------------
Note Time:         Note Text:                     

0:00:49            Abandoned lock: Java demo
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] To demonstrate what happens if you abandon a lock in Java, I'll be modifying a version of the previous dining philosophers example that I used to demonstrate a deadlock. It has three philosophers using chopsticks to take sushi from a shared plate. I've already fixed the deadlock in this version, so if I run this program, the philosophers successfully take turns eating sushi until all of the pieces are gone. The critical section for this program exists between the lock calls on line 23 and 23, and the calls to the unlock method on lines 32 and 33. Now, if one of the philosopher threads acquires those locks and then something goes wrong in the critical section and throws an unexpected error, that could cause its thread to terminate before it gets a chance to release those locks. To simulate that happening, I'm going to add another if statement that checks to see if there are exactly 10 pieces of sushi left. And if so, I'll use my favorite technique for intentionally crashing a thread, dividing by zero. Now, you should never intentionally divide by zero, but I'm doing it here to throw an exception that will cause Java to crash one of these threads. When I run this program, it gets all the way down to 10 pieces remaining. Then, the thread that happens to be executing at that time hits the divide by zero case, and crashes. In this run, Olivia was the unlucky thread. The other two threads are stuck waiting on the locks that Olivia will never release, so the program is stuck here forever. This scenario is slightly different than the deadlock we looked at previously because the threads are not waiting on each other to release the lock. But it's a related scenario and the impact is the same. This program isn't making any progress. So, to prevent this type of situation from occurring in Java, I should wrap the critical section in a try block. If I want, I can include any exception handling code in a catch statement after that. But what I really care about here is making sure that the locks always get released. And to do that, I'll add a finally block after the try statement, and I'll move the calls to unlock the chopsticks into it. Now when I run the program, an exception still occurs when Barron takes the 10th remaining piece of sushi, but thanks to the finally clause, Barron's thread is able to release the lock before terminating. I can see that after the 10th item, Olivia's thread took over to finish eating the rest of the sushi. This is a good practice to follow when using locks in Java. Always put the critical section in a try block and release the locks in a finally clause just in case something goes wrong. 

